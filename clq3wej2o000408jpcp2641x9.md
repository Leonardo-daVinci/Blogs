---
title: "Attention Mechanism"
seoTitle: "Attention Mechanism"
seoDescription: "Explore the need for Attention Mechanism, its differences from traditional Encoder-Decoder and how it improves Machine Translation task."
datePublished: Wed Dec 13 2023 15:00:10 GMT+0000 (Coordinated Universal Time)
cuid: clq3wej2o000408jpcp2641x9
slug: attention-mechanism
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1702394397627/43ba30b3-25b0-42d2-a6c8-e8d57bcfc828.png
tags: artificial-intelligence, machine-learning, google-cloud, learning, course, generative-ai, attention-mechanism

---

This is a collection of notes from [Attention Mechanism](https://www.cloudskillsboost.google/paths/183/course_templates/537) course by Google Cloud, taught by **Sanjana Reddy**. Some images are taken from the course itself.

It is a detailed compilation and annotated excerpts will be available on my [**LinkedIn profile**](https://www.linkedin.com/in/akshit-keoliya/).

Note that this article assumes you have basic understanding of how Encoder-Decoder works. For a refresher, check out my [Encoder-Decoder Architecture](https://keoliya.hashnode.dev/ed-arch) article.

# Course Overview

1. Need for Attention Mechanism
    
2. Attention Mechanism
    
3. How to Improve Translation
    

# Need for Attention Mechanism

The course explains the need of Attention Mechanism using the example use case of Machine Translation.

![Machine Translation image from Google Cloud](https://cdn.hashnode.com/res/hashnode/image/upload/v1702390119192/ead7cb66-d443-491e-8e4a-588c087198a8.png align="center")

For Machine Translation, we can use an Encoder-Decoder model (discussed in previous article) which takes one word at a time and translate them at each time step.  
The problem with this approach is that <mark>sequence of words doesn't always match between source and target language.</mark>

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1702390152622/a957cd21-c83a-461e-9256-8443c2bc35e5.png align="center")

Now, how to train the model to focus more on the word "**Cat**" than the word "**Black**" at first time step? This is where Attention Mechanism comes into picture.

# Attention Mechanism

Attention Mechanism is a technique that allows a Neural Network to <mark> focus on specific parts of input sequence</mark>. This is done by assigning different weights to different parts of the sequence with most important parts receiving the highest weights.

## Differences from Traditional Encoder-Decoder

### Traditional RNN based Encoder-Decoder

![Traditional Encoder-Decoder image from Google Cloud](https://cdn.hashnode.com/res/hashnode/image/upload/v1702390308444/3e3cdb4b-a65e-4f87-b5ac-a7538f095842.png align="center")

1. Model takes one word at a time as input, updates hidden state and passes on to next timestep.
    
2. In the end, only the final hidden state (Hidden state #3 above) is passed to the decoder.
    
3. Decoder works with this state for processing and translates to target language.
    

### Attention Model

Attention Model differs from the traditional model in two major regions:

1. **Passing more data to decoder.**
    
    ![Attention network image from Google Cloud](https://cdn.hashnode.com/res/hashnode/image/upload/v1702390454501/07c29e8f-7f53-4750-8680-f6abd9694dc5.png align="center")
    
    Instead of just passing the final hidden state to decoder, encoder passes all the hidden states from each timestep. This provides more context to decoder.
    
2. **Extra step before producing output.**
    
    ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1702390816653/24b61bdb-3a06-43c8-b790-78a71fbd8b56.png align="center")
    
    To focus only on more relevant parts, decoder does the following:
    
    1. Look at set of encoder hidden states that it has received.
        
    2. Give each hidden state a score.
        
    3. Multiply each hidden state by its soft-maxed score. This amplifies hidden state with highest score and downsizing states with low score.
        

# How to Improve Translation

Using the differences mentioned above, the course outlines the working of the Attention Network as follows:

![Attention Network image from Google Cloud](https://cdn.hashnode.com/res/hashnode/image/upload/v1702390976574/d2b05b3d-a14a-483e-adbb-fd2285b3ec24.png align="center")

Some notation used above:

1. "a<sub>t</sub>" represents attention weight at t-timestep.
    
2. "H<sub>t</sub>" represents hidden state of encoder at t-timestep.
    
3. "H<sub>dt</sub>" represents hidden state of decoder at t-timestep.
    

The process during attention step are as follows:

1. We use the encoder hidden states and H<sub>t</sub> vector to calculate context vector a<sub>t</sub> for "t" timestep. This is the weighted sum.
    
2. Then concatenate a<sub>t</sub> and H<sub>t</sub> into one vector.
    
3. Concatenated vector is passed to feedforward neural network, which is trained jointly with the model, to predict next word.
    
4. Output of the feedforward NN indicates output word for this timestep.
    
5. Process is continued until the End-of-Sentence token is generated by the decoder.
    

**Note:**  
We can see the inversion of words "Cat" and "Black" with attention mechanism. Also, we can see that word "Ate" translates into two words "a mange".  

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1702393260529/6e169737-7e8e-4a06-b613-a802f434ee6f.png align="center")

# Ending Note

We learnt about how Attention mechanism works and how it improves machine translation task. Next, we will learn about **Transformer** models and self-attention mechanism and how these are used for text classification, question answering, and natural language inference tasks.

Stay tuned for lots of Generative AI content!